# Autoencoder
This is the coursework produced for a Neural Computing course at City, University of London. I worked in MATLAB with the help of my fellow classmate Federico Cardoni. In the article we present a comparison of two neural networks applied to a digit recognition task on data from the widely-used MNIST dataset. We contrasted the Feedforward Multilayer Perceptron (MLP), a supervised learning algorithm, with a Stacked sparse Autoencoder (SAE), a deep network unsupervised one. Hyperparameters were varied in the training phase to achieve the best possible performance for each model, whose classification rate was compared on a separate test sample using Confusion Matrices. 
